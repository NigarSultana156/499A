{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NigarSultana156/499A/blob/main/objectDetectionRCNN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTnsRE3It_KB",
        "outputId": "f18576bf-3cb6-45c1-f205-4ca2c1a6502c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvJJiq1xuQh9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s79TPwE4uQvl",
        "outputId": "66e26ccd-9644-4c19-c42d-e2164077522e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files extracted to: /content/extracted_files and /content/extracted_files\n"
          ]
        }
      ],
      "source": [
        "# Paths for videos and dataset\n",
        "video_paths = [\n",
        "    '/content/drive/MyDrive/top-view-multi-person-tracking-2020/cam1_1hour.mp4',\n",
        "    '/content/drive/MyDrive/top-view-multi-person-tracking-2020/cam2_1hour.mp4',\n",
        "    '/content/drive/MyDrive/top-view-multi-person-tracking-2020/cam3_1hour.mp4',\n",
        "    '/content/drive/MyDrive/top-view-multi-person-tracking-2020/cam5_1hour.mp4',\n",
        "    '/content/drive/MyDrive/top-view-multi-person-tracking-2020/cam6_1hour.mp4'\n",
        "]\n",
        "\n",
        "# Extract ZIP file for labeled images\n",
        "zip_file_path = '/content/drive/MyDrive/top-view-multi-person-tracking-2020/labeled_images/train.zip'\n",
        "extraction_path = '/content/extracted_files'\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "test_zip_file_path = '/content/drive/MyDrive/top-view-multi-person-tracking-2020/labeled_images/test.zip'\n",
        "test_extraction_path = '/content/extracted_files'\n",
        "os.makedirs(test_extraction_path, exist_ok=True)\n",
        "with zipfile.ZipFile(test_zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(test_extraction_path)\n",
        "\n",
        "print(\"Files extracted to:\", extraction_path, \"and\", test_extraction_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dDCnkF8uQy9"
      },
      "outputs": [],
      "source": [
        "# Load Faster R-CNN model from TensorFlow Hub\n",
        "model = hub.load(\"https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxEHXPZMuQ7E"
      },
      "outputs": [],
      "source": [
        "def detect_objects(frame):\n",
        "    # Convert the frame to a tensor\n",
        "    input_tensor = tf.convert_to_tensor([frame], dtype=tf.uint8)\n",
        "\n",
        "    # Run detection\n",
        "    detections = model(input_tensor)\n",
        "\n",
        "    # Process the detection results\n",
        "    boxes = detections[\"detection_boxes\"].numpy()[0]\n",
        "    scores = detections[\"detection_scores\"].numpy()[0]\n",
        "    classes = detections[\"detection_classes\"].numpy()[0]\n",
        "\n",
        "    # Filter detections with a high confidence score and only keep persons (class 1)\n",
        "    detection_threshold = 0.5\n",
        "    filtered_boxes = boxes[(scores >= detection_threshold) & (classes == 1)]\n",
        "    filtered_scores = scores[(scores >= detection_threshold) & (classes == 1)]\n",
        "\n",
        "    # Draw bounding boxes on the frame and add counts and confidence scores\n",
        "    for i, box in enumerate(filtered_boxes):\n",
        "        ymin, xmin, ymax, xmax = box\n",
        "        start_point = (int(xmin * frame.shape[1]), int(ymin * frame.shape[0]))\n",
        "        end_point = (int(xmax * frame.shape[1]), int(ymax * frame.shape[0]))\n",
        "        frame = cv2.rectangle(frame, start_point, end_point, (0, 255, 0), 3)  # Green box, thickness 3\n",
        "\n",
        "        # Add count and confidence score as text\n",
        "        count_text = str(i + 1)\n",
        "        confidence_text = f\"{filtered_scores[i]:.2f}\"\n",
        "        cv2.putText(frame, count_text, (start_point[0], start_point[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        cv2.putText(frame, confidence_text, (start_point[0], start_point[1] - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    return frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o-n7HemuQ-k"
      },
      "outputs": [],
      "source": [
        "# Initialize video captures\n",
        "captures = [cv2.VideoCapture(video_path) for video_path in video_paths]\n",
        "for capture in captures:\n",
        "    assert capture.isOpened()\n",
        "\n",
        "im_width = int(captures[0].get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "im_height = int(captures[0].get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "out_size = (im_width * 3, im_height * 2)\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv2.VideoWriter('/content/output_with_detection.avi', fourcc, 20, out_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlzvJpDDuRHB",
        "outputId": "704c2edc-8408-494b-f2dd-a84b1ff15aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed frame 0 of 500\n",
            "Processed frame 100 of 500\n",
            "Processed frame 200 of 500\n"
          ]
        }
      ],
      "source": [
        "num_frames = 500  # Set a lower number of frames to process\n",
        "frame_skip = 2    # Skip every 2 frames to reduce processing frequency\n",
        "\n",
        "current_frame = 0\n",
        "\n",
        "while current_frame < num_frames:\n",
        "    frames = []\n",
        "    for capture in captures:\n",
        "        ret, frame = capture.read()\n",
        "        if not ret:  # Exit if the frame read was not successful\n",
        "            frames.append(None)\n",
        "        else:\n",
        "            frames.append(frame)\n",
        "\n",
        "    if any(frame is None for frame in frames):\n",
        "        break  # Exit if any video ends\n",
        "\n",
        "    # Apply detection on each frame\n",
        "    frames = [detect_objects(frame) for frame in frames]\n",
        "\n",
        "    shape1 = list(frames[0].shape)\n",
        "    shape1[1] = 160\n",
        "    shape2 = list(frames[0].shape)\n",
        "    shape2[1] = 1280 - 160\n",
        "\n",
        "    upper = np.hstack((np.zeros(shape1), frames[3], frames[4], np.zeros(shape2)))\n",
        "    lower = np.hstack((frames[0], frames[1], frames[2]))\n",
        "\n",
        "    merged_frame = np.vstack((upper, lower))\n",
        "    out.write(np.uint8(merged_frame))\n",
        "\n",
        "    if current_frame % 100 == 0:\n",
        "        print(\"Processed frame %d of %d\" % (current_frame, num_frames))\n",
        "\n",
        "    current_frame += frame_skip  # Increment by frame_skip to process fewer frames\n",
        "\n",
        "out.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2WqzosXzR3F"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example data\n",
        "frame_indices = list(range(1, 501))  # Frame indices from 1 to 500\n",
        "detection_counts = [np.random.randint(0, 5) for _ in frame_indices]  # Random detection counts for demonstration\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(frame_indices, detection_counts, marker='o', linestyle='-', color='b')\n",
        "plt.title('Number of Person Detections Over Time')\n",
        "plt.xlabel('Frame Number')\n",
        "plt.ylabel('Number of Detections')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suwDbOiqzT_k"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Example confidence scores (random for demonstration)\n",
        "confidence_scores = [np.random.rand() for _ in range(1000)]  # Generate 1000 random confidence scores\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(confidence_scores, bins=20, kde=True)\n",
        "plt.title('Distribution of Confidence Scores for Person Detections')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMkUl5CwzT8G"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Example precision and recall data (random for demonstration)\n",
        "y_true = np.random.randint(0, 2, size=1000)  # Random binary true labels\n",
        "y_scores = np.random.rand(1000)  # Random scores\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(recall, precision, marker='o', linestyle='-', color='g')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K7jQhS3zyjl"
      },
      "outputs": [],
      "source": [
        "# Function to visualize detected bounding boxes on an image\n",
        "def visualize_detections(image, boxes, scores, classes):\n",
        "    for i, box in enumerate(boxes):\n",
        "        ymin, xmin, ymax, xmax = box\n",
        "        start_point = (int(xmin * image.shape[1]), int(ymin * image.shape[0]))\n",
        "        end_point = (int(xmax * image.shape[1]), int(ymax * image.shape[0]))\n",
        "        cv2.rectangle(image, start_point, end_point, (0, 255, 0), 3)  # Green box\n",
        "        cv2.putText(image, f'{scores[i]:.2f}', (start_point[0], start_point[1] - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "    return image\n",
        "\n",
        "# Example of visualizing the first detected frame\n",
        "detected_frame = visualize_detections(cam1, filtered_boxes, filtered_scores, filtered_classes)  # Assuming these variables exist\n",
        "plt.imshow(cv2.cvtColor(detected_frame, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.title('Detected Persons in Frame')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLtp0MlvuwQy"
      },
      "outputs": [],
      "source": [
        "# Convert to MP4 format\n",
        "!ffmpeg -i /content/output_with_detection.avi -vcodec libx264 /content/output_with_detection.mp4\n",
        "\n",
        "# Download the output video\n",
        "from google.colab import files\n",
        "files.download('/content/output_with_detection.mp4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF8l9xBPuwNU"
      },
      "outputs": [],
      "source": [
        "def process_new_video(video_path, output_path='/content/detected_new_video_output.avi'):\n",
        "    capture = cv2.VideoCapture(video_path)\n",
        "    assert capture.isOpened()\n",
        "\n",
        "    im_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    im_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 20, (im_width, im_height))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = capture.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        detected_frame = detect_objects(frame)\n",
        "        out.write(np.uint8(detected_frame))\n",
        "\n",
        "    capture.release()\n",
        "    out.release()\n",
        "    print(f\"Output video saved as {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHrE_KphuwJz"
      },
      "outputs": [],
      "source": [
        "# Test with a new video\n",
        "new_video_path = '/content/drive/MyDrive/path_to_your_new_video.mp4'  # Update with the path of the new video file\n",
        "process_new_video(new_video_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf4l3jc_uwGc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}