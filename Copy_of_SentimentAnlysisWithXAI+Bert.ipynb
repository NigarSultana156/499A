{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NigarSultana156/499A/blob/main/Copy_of_SentimentAnlysisWithXAI%2BBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEcji_7cK0zs"
      },
      "source": [
        "# 1. Install Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1E5mWsOH-KA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch torch-optimizer imbalanced-learn scikit-learn matplotlib --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5EaO7JvLe8y"
      },
      "source": [
        "#2. Load and Preprocess The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfAOXKaWKRIh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your scraped data from the provided URL\n",
        "url = \"https://raw.githubusercontent.com/amanullahshah32/Review-Scraping/refs/heads/main/Dataset/cleaned_dataset.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Drop rows where 'review_description' or 'rating' are missing\n",
        "df.dropna(subset=['review_description', 'rating'], inplace=True)\n",
        "\n",
        "# Shuffle the entire dataset (no sampling)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Create a sentiment column based on rating (1-2 -> Negative, 3 -> Neutral, 4-5 -> Positive)\n",
        "df['sentiment'] = df['rating'].apply(lambda x: 0 if x <= 2 else (1 if x == 3 else 2))\n",
        "\n",
        "# Split the data into training and validation sets (80% training, 20% validation)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['review_description'], df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert labels to lists\n",
        "train_labels = train_labels.tolist()\n",
        "val_labels = val_labels.tolist()\n",
        "\n",
        "# Display the first few rows of the shuffled dataset\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWEr9nyXL4Kk"
      },
      "source": [
        "#3.Handle Class Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e0O7bxKKRFR"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Initialize RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Since train_texts is a pandas Series, we need to reshape it to a DataFrame\n",
        "train_texts_df = pd.DataFrame(train_texts)\n",
        "\n",
        "# Apply oversampling to balance the classes\n",
        "train_texts_resampled, train_labels_resampled = ros.fit_resample(train_texts_df, train_labels)\n",
        "\n",
        "# Convert the DataFrame of resampled texts back to a list\n",
        "train_texts_resampled = train_texts_resampled.squeeze().tolist()  # .squeeze() ensures a flat list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0kB6QxdML4r"
      },
      "source": [
        "#4. Tokenization with BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkkz2nXkKRCU"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text data\n",
        "train_encodings = tokenizer(train_texts_resampled, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdm373srMPwJ"
      },
      "source": [
        "#5. Create a Dataset Class for PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9BfMmJ8KQ-z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create the PyTorch datasets\n",
        "train_dataset = ReviewDataset(train_encodings, train_labels_resampled)\n",
        "val_dataset = ReviewDataset(val_encodings, val_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO_BMLK5MTv5"
      },
      "source": [
        "#6. Load Pre-trained BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZI3CowORevo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Define the device (use GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification (3 classes)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpNjEyjoMlD7"
      },
      "source": [
        "#7. Set Up DataLoader, Optimizer, and Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF_emcv1KQ3x"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Optimizer: AdamW with weight decay and a smaller learning rate\n",
        "learning_rate = 3e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "# Scheduler for learning rate decay\n",
        "epochs = 5\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXqcvpzGMquy"
      },
      "source": [
        "#8. Class Weights for Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyXeDtn6KoHD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Convert the class labels to a NumPy array\n",
        "classes = np.array([0, 1, 2])\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=train_labels_resampled)\n",
        "\n",
        "# Convert to a PyTorch tensor and move it to the appropriate device\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Use the weights in the loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8ZDL5_IMyzi"
      },
      "source": [
        "#9. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "35m0mC39KoEK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize lists to track metrics\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "epoch_durations = []\n",
        "\n",
        "# Loop for training and validation\n",
        "for epoch in range(5):  # Training for 10 epochs\n",
        "    start_time = time.time()  # Start time for the epoch\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    train_preds = []\n",
        "    train_labels_epoch = []  # Track labels for each epoch\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Collect predictions\n",
        "        train_preds.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n",
        "        train_labels_epoch.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    end_time = time.time()  # End time for the epoch\n",
        "    epoch_duration = end_time - start_time  # Time taken for the epoch\n",
        "    epoch_durations.append(epoch_duration)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_acc = accuracy_score(train_labels_epoch, train_preds)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            val_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f'Epoch {epoch+1} completed in {epoch_duration:.2f} seconds')\n",
        "    print(f'Training Accuracy: {train_acc:.4f}')\n",
        "    print(f'Validation Accuracy: {val_acc:.4f}')\n",
        "\n",
        "    # Classification report\n",
        "    print(f'Classification Report (Validation):\\n {classification_report(val_labels, val_preds)}')\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdWqTp_qM5Qp"
      },
      "source": [
        "#10. Make New Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx_FZxzXKoBL"
      },
      "outputs": [],
      "source": [
        "# Make predictions on new data (Example: a list of review texts)\n",
        "new_reviews = [\n",
        "    \"The app is very helpful for tracking my health.\",\n",
        "    \"I had a bad experience, it kept crashing.\",\n",
        "    \"Great app, I would definitely recommend it to others!\"\n",
        "]\n",
        "\n",
        "# Tokenize the new reviews\n",
        "new_encodings = tokenizer(new_reviews, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Move the tensors to the appropriate device\n",
        "new_encodings = {key: val.to(device) for key, val in new_encodings.items()}\n",
        "\n",
        "# Perform the prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**new_encodings)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "# Print the predictions (0 = Negative, 1 = Neutral, 2 = Positive)\n",
        "for review, pred in zip(new_reviews, predictions):\n",
        "    sentiment = ['Negative', 'Neutral', 'Positive'][pred]\n",
        "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFnU67yxHT2s"
      },
      "outputs": [],
      "source": [
        "!pip install shap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnjhFBXiM-az"
      },
      "source": [
        "#11. SHAP for Model Explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKvTkJWFYbUy"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Ensure the model is in evaluation mode and on the correct device (GPU if available)\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define a function for tokenizing text\n",
        "def tokenize_text(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(\n",
        "        list(texts),  # Ensure texts are in the correct format\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenizer instance\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# A wrapper function for SHAP to pass through the model\n",
        "def predict(texts):\n",
        "    inputs = tokenize_text(texts, tokenizer)\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Ensure inputs are on the same device as the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        return torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "# SHAP Explainer setup\n",
        "explainer = shap.Explainer(predict, tokenizer)\n",
        "\n",
        "# Select a few samples from your validation set to explain\n",
        "texts_to_explain = [str(text) for text in val_texts[:5]]  # Explicitly ensure a list of strings\n",
        "shap_values = explainer(texts_to_explain)\n",
        "\n",
        "# Visualize the explanations\n",
        "for i, text in enumerate(texts_to_explain):\n",
        "    print(f\"\\n--- Explanation for Text {i+1}: ---\")\n",
        "    print(text)\n",
        "    shap.text_plot(shap_values[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjmFzGEKRA9Y"
      },
      "source": [
        "#12. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCRfSnfolMht"
      },
      "outputs": [],
      "source": [
        "# Basic overview of the dataset\n",
        "print(df.info())  # Column types and non-null counts\n",
        "print(df.describe())  # Summary statistics for numerical columns\n",
        "print(df.head())  # Preview the first few rows\n",
        "print(df['sentiment'].value_counts())  # Distribution of sentiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOBtUucJlTnV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot the sentiment distribution\n",
        "sns.countplot(data=df, x='sentiment', palette='Set2')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(ticks=[0, 1, 2], labels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G7XWIzGO2kS"
      },
      "outputs": [],
      "source": [
        "# Add a column for word counts\n",
        "df['word_count'] = df['review_description'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Plot word count distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['word_count'], bins=50, kde=True, color='blue')\n",
        "plt.title('Distribution of Word Counts in Reviews')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Compare word counts across sentiments\n",
        "sns.boxplot(data=df, x='sentiment', y='word_count', palette='Set3')\n",
        "plt.title('Word Counts by Sentiment')\n",
        "plt.xticks(ticks=[0, 1, 2], labels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Word Count')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcAc5Um0ljG8"
      },
      "outputs": [],
      "source": [
        "# Sentiment distribution for each rating\n",
        "rating_sentiment = df.groupby('rating')['sentiment'].value_counts().unstack()\n",
        "rating_sentiment.plot(kind='bar', stacked=True, figsize=(10, 6), cmap='coolwarm')\n",
        "plt.title('Sentiment Distribution Across Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Sentiment', labels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUAOEtsYli_a"
      },
      "outputs": [],
      "source": [
        "# Average rating by app version\n",
        "version_rating = df.groupby('appVersion')['rating'].mean().sort_values()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "version_rating.plot(kind='line', marker='o', color='purple')\n",
        "plt.title('Average Rating by App Version')\n",
        "plt.xlabel('App Version')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Feh7aQ41li4s"
      },
      "outputs": [],
      "source": [
        "# Create a column indicating whether a developer responded\n",
        "df['has_dev_response'] = df['developer_response'].notnull()\n",
        "\n",
        "# Compare average rating for apps with and without developer responses\n",
        "response_rating = df.groupby('has_dev_response')['rating'].mean()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "response_rating.plot(kind='bar', color=['red', 'green'])\n",
        "plt.title('Average Rating with/without Developer Response')\n",
        "plt.xlabel('Developer Response')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(ticks=[0, 1], labels=['No Response', 'Response'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWVjZ-g5lipN"
      },
      "outputs": [],
      "source": [
        "# Top 10 apps by review count\n",
        "top_apps = df['app_name'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_apps.plot(kind='bar', color='skyblue')\n",
        "plt.title('Top 10 Apps by Number of Reviews')\n",
        "plt.xlabel('App Name')\n",
        "plt.ylabel('Review Count')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqc7ONXqmCw9"
      },
      "outputs": [],
      "source": [
        "# Sentiment distribution per app\n",
        "sentiment_per_app = df.groupby('app_name')['sentiment'].value_counts().unstack()\n",
        "\n",
        "# Plot for the top 5 apps by review count\n",
        "top_5_apps = df['app_name'].value_counts().head(5).index\n",
        "sentiment_per_app.loc[top_5_apps].plot(kind='bar', stacked=True, figsize=(12, 6), cmap='viridis')\n",
        "plt.title('Sentiment Distribution for Top 5 Apps')\n",
        "plt.xlabel('App Name')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Sentiment', labels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq0gHhSQn1M0"
      },
      "outputs": [],
      "source": [
        " #Monthly Sentiment Trend\n",
        "df['review_date'] = pd.to_datetime(df['review_date'])\n",
        "df['month'] = df['review_date'].dt.to_period('M')\n",
        "monthly_sentiment = df.groupby(['month', 'sentiment']).size().unstack().fillna(0)\n",
        "monthly_sentiment.plot(kind='line', figsize=(12, 6))\n",
        "plt.title('Sentiment Trend Over Time (Monthly)')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie-fXJw9mnZJ"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Generate word clouds for each sentiment\n",
        "for sentiment in [0, 1, 2]:\n",
        "    text = \" \".join(df[df['sentiment'] == sentiment]['review_description'])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Word Cloud for Sentiment: {\"Negative\" if sentiment == 0 else \"Neutral\" if sentiment == 1 else \"Positive\"}')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQIpb80ao082"
      },
      "outputs": [],
      "source": [
        "#Review Time and Sentiment:\n",
        "df['hour'] = df['review_date'].dt.hour\n",
        "sentiment_by_hour = df.groupby('hour')['sentiment'].value_counts().unstack().fillna(0)\n",
        "sentiment_by_hour.plot(kind='line', figsize=(12, 6))\n",
        "plt.title('Sentiment Distribution by Hour of the Day')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZihDvBnpDeL"
      },
      "outputs": [],
      "source": [
        "#Sentiment Distribution by Review Content (Negative Keywords):\n",
        "negative_keywords = ['bug', 'crash', 'slow', 'problem', 'issue']\n",
        "df['negative_keywords_count'] = df['review_description'].apply(lambda x: sum(1 for word in negative_keywords if word in str(x).lower()))\n",
        "sentiment_by_keywords = df.groupby('negative_keywords_count')['sentiment'].value_counts().unstack().fillna(0)\n",
        "sentiment_by_keywords.plot(kind='line', figsize=(12, 6))\n",
        "plt.title('Sentiment vs Negative Keywords in Reviews')\n",
        "plt.xlabel('Number of Negative Keywords')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt4gAMSvpWTk"
      },
      "outputs": [],
      "source": [
        "#Sentiment Distribution Based on Review Source:\n",
        "sentiment_by_source = df.groupby('source')['sentiment'].value_counts().unstack().fillna(0)\n",
        "sentiment_by_source.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
        "plt.title('Sentiment Distribution by Review Source')\n",
        "plt.xlabel('Source (Play Store or App Store)')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mnzKMXephNk"
      },
      "outputs": [],
      "source": [
        "#Top Reviewers:\n",
        "top_reviewers = df['user_name'].value_counts().head(10)\n",
        "top_reviewers.plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Top 10 Reviewers by Number of Reviews')\n",
        "plt.xlabel('User Name')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzsbAnwPplbM"
      },
      "outputs": [],
      "source": [
        "#Sentiment Distribution vs Thumbs Up\n",
        "df['thumbs_up'].groupby(df['sentiment']).mean().plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Average Thumbs Up per Sentiment')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Average Thumbs Up')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWTvGVGl0PxA"
      },
      "source": [
        "#13. Save and Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtAejAvpqArs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the save directory\n",
        "save_directory = \"./saved_bert_model\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {save_directory}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGp9yDj2sSkj"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Load the model and tokenizer\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(save_directory)\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD_pK6b1z2Ea"
      },
      "source": [
        "#14. Bleu Score (not a ideal metric for sentiment analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm45RosDx4JS"
      },
      "outputs": [],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpGyQvC5yYCQ"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Example of your sentiment analysis model setup (BERT model)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a function for tokenizing text\n",
        "def tokenize_text(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(\n",
        "        list(texts),  # Ensure texts are in the correct format\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Your sentiment analysis prediction function\n",
        "def predict(texts):\n",
        "    inputs = tokenize_text(texts, tokenizer)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Assuming a simple classification problem with two classes (positive/negative)\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "        return preds.cpu().numpy()\n",
        "\n",
        "# Function to calculate BLEU score (for text generation)\n",
        "def calculate_bleu(reference_texts, generated_texts):\n",
        "    reference = [text.split() for text in reference_texts]\n",
        "    hypothesis = [text.split() for text in generated_texts]\n",
        "\n",
        "    bleu_scores = []\n",
        "    for ref, hyp in zip(reference, hypothesis):\n",
        "        bleu_score = sentence_bleu([ref], hyp)  # For 1-gram BLEU score\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    return bleu_scores\n",
        "\n",
        "# Example input (sentiment labels)\n",
        "texts = [\"This is a great product!\", \"This is a terrible product!\"]\n",
        "\n",
        "# Reference texts (in a text generation task, these might be manually written summaries)\n",
        "reference_texts = [\"positive sentiment\", \"negative sentiment\"]\n",
        "\n",
        "# Generate predictions (class labels, in your case)\n",
        "predictions = predict(texts)\n",
        "\n",
        "# For simplicity, let's assume that our model outputs sentiment labels and we convert them to text\n",
        "generated_texts = [\"positive sentiment\" if p == 1 else \"negative sentiment\" for p in predictions]\n",
        "\n",
        "# Calculate BLEU score (this assumes the model's output is a generated summary or sentiment text)\n",
        "bleu_scores = calculate_bleu(reference_texts, generated_texts)\n",
        "print(f\"BLEU Score: {bleu_scores}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeNWpfp6zK4Q"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Example of applying smoothing to BLEU calculation\n",
        "def calculate_bleu_with_smoothing(reference_texts, generated_texts):\n",
        "    reference = [text.split() for text in reference_texts]\n",
        "    hypothesis = [text.split() for text in generated_texts]\n",
        "\n",
        "    smoothing_function = SmoothingFunction().method1  # Use a smoothing method\n",
        "    bleu_scores = []\n",
        "    for ref, hyp in zip(reference, hypothesis):\n",
        "        bleu_score = sentence_bleu([ref], hyp, smoothing_function=smoothing_function)  # Apply smoothing\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    return bleu_scores\n",
        "\n",
        "# Use the same function as before to calculate BLEU with smoothing\n",
        "bleu_scores = calculate_bleu_with_smoothing(reference_texts, generated_texts)\n",
        "print(f\"BLEU Score with Smoothing: {bleu_scores}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOIbE8xRb3MFOfWlZkgpnPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}